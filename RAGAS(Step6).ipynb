{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "438938a21b5943678d22b0cb26e041ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81529945392a4d26b10fb2c6b048eed3",
              "IPY_MODEL_f51784bad9594219837bd04ad7866a92",
              "IPY_MODEL_2cb4aa58a5ae461ca7a07ebc6e6103d0"
            ],
            "layout": "IPY_MODEL_30b402a7864d4f29b4137b4c5959fcd5"
          }
        },
        "81529945392a4d26b10fb2c6b048eed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92222202ca2447ce8c32eafcea71c50f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_91c9848b65c3438b97e7331d8d0f541c",
            "value": "Batches:â€‡100%"
          }
        },
        "f51784bad9594219837bd04ad7866a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60610f5109684fd89f4f0ddd779dd2be",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb9a9ba7fbb74bb488da4a52551e6651",
            "value": 21
          }
        },
        "2cb4aa58a5ae461ca7a07ebc6e6103d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5f475357574cc8a83c19ed7526aa67",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ea2c99c14e364cb2a6a88ab83783d063",
            "value": "â€‡21/21â€‡[00:57&lt;00:00,â€‡â€‡1.15s/it]"
          }
        },
        "30b402a7864d4f29b4137b4c5959fcd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92222202ca2447ce8c32eafcea71c50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91c9848b65c3438b97e7331d8d0f541c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60610f5109684fd89f4f0ddd779dd2be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9a9ba7fbb74bb488da4a52551e6651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b5f475357574cc8a83c19ed7526aa67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2c99c14e364cb2a6a88ab83783d063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b43d966235c24d528369292249cf2a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc2e72b0c83741ebb94830831be9eb8d",
              "IPY_MODEL_7fc5661323a649f39049e9c37c93b3dc",
              "IPY_MODEL_3b7090357986445ba1c752505eae6984"
            ],
            "layout": "IPY_MODEL_9e3832000a4744ceb8d4c301bd672604"
          }
        },
        "cc2e72b0c83741ebb94830831be9eb8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34c93a87ac884d39945ea8990e9a96f9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0cef774434234231b97a57d388ac0ab1",
            "value": "Batches:â€‡100%"
          }
        },
        "7fc5661323a649f39049e9c37c93b3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2f21720f8f5415e98b5e5643e374c08",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32e3f3ed923b416a83b3dc91f6ca8bb7",
            "value": 21
          }
        },
        "3b7090357986445ba1c752505eae6984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62d50488bba743a3a7337e5ddd862480",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8c0bdee8c2244b6f81332f5728ce6f30",
            "value": "â€‡21/21â€‡[00:56&lt;00:00,â€‡â€‡1.03s/it]"
          }
        },
        "9e3832000a4744ceb8d4c301bd672604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34c93a87ac884d39945ea8990e9a96f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cef774434234231b97a57d388ac0ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2f21720f8f5415e98b5e5643e374c08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32e3f3ed923b416a83b3dc91f6ca8bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62d50488bba743a3a7337e5ddd862480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c0bdee8c2244b6f81332f5728ce6f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment & Installs"
      ],
      "metadata": {
        "id": "HCACy1eRedba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safe to rerun\n",
        "import sys, subprocess\n",
        "\n",
        "def _pip(*args):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *args], check=True)\n",
        "\n",
        "_pip(\n",
        "    \"datasets\", \"evaluate\", \"pandas\",\n",
        "    \"ragas>=0.1.9\",          # optional; not required for local ID-overlap metrics\n",
        "    \"transformers\", \"sentence-transformers\",\n",
        "    \"faiss-cpu\", \"nbconvert\",\n",
        "    \"pymilvus\"               # not used here but handy if upstream notebooks used Milvus\n",
        ")\n",
        "\n",
        "import os, re, importlib, runpy, types\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "TokpY6nQed2N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount (Colab) or use local CWD\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\")\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Try both styles Colab has used over time\n",
        "CANDIDATE_ROOTS = [Path(\"/content/drive/MyDrive\"), Path(\"/content/drive/My Drive\")]\n",
        "if not IN_COLAB:\n",
        "    CANDIDATE_ROOTS = [Path.cwd()]  # local dev\n",
        "\n",
        "TARGET_FILENAMES = {\n",
        "    \"step2\": \"NAIVE_RAG(Steps_2_and_3).ipynb\",\n",
        "    \"step5\": \"AdvancedRAG(Step5).ipynb\",\n",
        "}\n",
        "\n",
        "def find_notebook(filename: str) -> Path | None:\n",
        "    for root in CANDIDATE_ROOTS:\n",
        "        if not root.exists():\n",
        "            continue\n",
        "        # First, check your stated folder\n",
        "        nb_dir = root / \"NLX_LLM_Project_2\"\n",
        "        candidate = nb_dir / filename\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "        # If not there, search the whole Drive (can take a few seconds)\n",
        "        for p in root.rglob(\"*.ipynb\"):\n",
        "            if p.name == filename:\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "NB_STEP2 = find_notebook(TARGET_FILENAMES[\"step2\"])\n",
        "NB_STEP5 = find_notebook(TARGET_FILENAMES[\"step5\"])\n",
        "\n",
        "print(\"Resolved Step2 path:\", NB_STEP2)\n",
        "print(\"Resolved Step5 path:\", NB_STEP5)\n",
        "\n",
        "# Where to write converted .py and outputs\n",
        "if IN_COLAB and (NB_STEP2 or NB_STEP5):\n",
        "    # Prefer placing artifacts beside the notebooks if possible\n",
        "    ARTIFACTS = (NB_STEP2 or NB_STEP5).parent / \"artifacts\"\n",
        "else:\n",
        "    ARTIFACTS = Path.cwd() / \"artifacts\"\n",
        "\n",
        "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Artifacts dir:\", ARTIFACTS)\n",
        "\n",
        "# Quick sanity: list the target folder if a path is missing\n",
        "if NB_STEP2 is None or NB_STEP5 is None:\n",
        "    for root in CANDIDATE_ROOTS:\n",
        "        test_dir = root / \"NLX_LLM_Project_2\"\n",
        "        if test_dir.exists():\n",
        "            print(\"Contents of\", test_dir, \":\\n\", [p.name for p in test_dir.iterdir()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCssLtC9etKY",
        "outputId": "79085d81-c407-4729-8c61-fadbf8607059"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Resolved Step2 path: /content/drive/MyDrive/Colab Notebooks/NLX_LLM_Project_2/NAIVE_RAG(Steps_2_and_3).ipynb\n",
            "Resolved Step5 path: /content/drive/MyDrive/Colab Notebooks/AdvancedRAG(Step5).ipynb\n",
            "Artifacts dir: /content/drive/MyDrive/Colab Notebooks/NLX_LLM_Project_2/artifacts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nbconvert import PythonExporter\n",
        "import nbformat\n",
        "\n",
        "def convert_notebook(nb_path: Path, out_path: Path):\n",
        "    assert nb_path is not None, \"Notebook path is None (not found).\"\n",
        "    assert nb_path.exists(), f\"Missing notebook: {nb_path}\"\n",
        "    nb = nbformat.read(nb_path, as_version=4)\n",
        "    code, _ = PythonExporter().from_notebook_node(nb)\n",
        "    out_path.write_text(code, encoding=\"utf-8\")\n",
        "    return out_path\n",
        "\n",
        "STEP2_PY = ARTIFACTS / \"step2_module.py\"\n",
        "STEP5_PY = ARTIFACTS / \"step5_module.py\"\n",
        "\n",
        "convert_notebook(NB_STEP2, STEP2_PY)\n",
        "convert_notebook(NB_STEP5, STEP5_PY)\n",
        "\n",
        "print(\"Converted to:\", STEP2_PY, \"and\", STEP5_PY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAzffkoAf0uj",
        "outputId": "36de52a5-a781-4c55-ce97-f7ddc0eabb78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted to: /content/drive/MyDrive/Colab Notebooks/NLX_LLM_Project_2/artifacts/step2_module.py and /content/drive/MyDrive/Colab Notebooks/NLX_LLM_Project_2/artifacts/step5_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 â€” Bind from your notebooks (adapts `chunks` + `content` â†’ canonical `chunk_store`)\n",
        "\n",
        "import importlib.util, types, runpy\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "def load_module_from_path(name: str, path: Path) -> types.ModuleType:\n",
        "    spec = importlib.util.spec_from_file_location(name, str(path))\n",
        "    mod = importlib.util.module_from_spec(spec)  # type: ignore\n",
        "    assert spec and spec.loader, f\"Cannot load {name} from {path}\"\n",
        "    spec.loader.exec_module(mod)  # type: ignore\n",
        "    return mod\n",
        "\n",
        "mod_step2 = load_module_from_path(\"step2_module\", STEP2_PY)\n",
        "mod_step5 = load_module_from_path(\"step5_module\", STEP5_PY)\n",
        "\n",
        "def to_chunk_store(raw) -> List[Dict[str, str]]:\n",
        "    out = []\n",
        "    if isinstance(raw, list):\n",
        "        for i, x in enumerate(raw):\n",
        "            if isinstance(x, dict):\n",
        "                txt = x.get(\"text\") or x.get(\"body\") or x.get(\"content\") or x.get(\"chunk\")\n",
        "                if isinstance(txt, str) and txt.strip():\n",
        "                    out.append({\"id\": str(x.get(\"id\", i)), \"text\": txt})\n",
        "            elif isinstance(x, str) and x.strip():\n",
        "                out.append({\"id\": str(i), \"text\": x})\n",
        "    elif isinstance(raw, pd.DataFrame):\n",
        "        col = next((c for c in (\"text\",\"body\",\"content\",\"chunk\") if c in raw.columns), None)\n",
        "        if col:\n",
        "            ser = raw[col].astype(str).fillna(\"\")\n",
        "            out = [{\"id\": str(i), \"text\": t} for i, t in ser.items() if t.strip()]\n",
        "    return out\n",
        "\n",
        "# Prefer Step 5 exports, then Step 2\n",
        "raw_docs = (\n",
        "    getattr(mod_step5, \"docs\", None) or\n",
        "    getattr(mod_step5, \"chunks\", None) or\n",
        "    getattr(mod_step2, \"docs\", None) or\n",
        "    getattr(mod_step2, \"chunks\", None)\n",
        ")\n",
        "\n",
        "# If import didnâ€™t execute main blocks, run once with runpy as a fallback\n",
        "if not raw_docs:\n",
        "    ns2 = runpy.run_path(str(STEP2_PY))\n",
        "    ns5 = runpy.run_path(str(STEP5_PY))\n",
        "    raw_docs = (\n",
        "        ns5.get(\"docs\") or ns5.get(\"chunks\") or\n",
        "        ns2.get(\"docs\") or ns2.get(\"chunks\")\n",
        "    )\n",
        "\n",
        "chunk_store = to_chunk_store(raw_docs)\n",
        "assert chunk_store, \"Couldnâ€™t normalize any docs/chunks into {id,text}.\"\n",
        "\n",
        "# Build embedder / FAISS index if not exported by Step 5\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "embed_model = getattr(mod_step5, \"embed_model\", None)\n",
        "faiss_index = getattr(mod_step5, \"index\", None)\n",
        "\n",
        "if embed_model is None:\n",
        "    embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "if faiss_index is None:\n",
        "    vecs = embed_model.encode([d[\"text\"] for d in chunk_store], normalize_embeddings=True).astype(\"float32\")\n",
        "    faiss_index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "    faiss_index.add(vecs)\n",
        "\n",
        "print(f\"[OK] chunks={len(chunk_store)} | embedder={type(embed_model).__name__} | faiss_ntotal={faiss_index.ntotal}\")\n",
        "print(\"Sample:\", chunk_store[0])"
      ],
      "metadata": {
        "id": "JYaXXKvrhjR8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "438938a21b5943678d22b0cb26e041ee",
            "81529945392a4d26b10fb2c6b048eed3",
            "f51784bad9594219837bd04ad7866a92",
            "2cb4aa58a5ae461ca7a07ebc6e6103d0",
            "30b402a7864d4f29b4137b4c5959fcd5",
            "92222202ca2447ce8c32eafcea71c50f",
            "91c9848b65c3438b97e7331d8d0f541c",
            "60610f5109684fd89f4f0ddd779dd2be",
            "eb9a9ba7fbb74bb488da4a52551e6651",
            "4b5f475357574cc8a83c19ed7526aa67",
            "ea2c99c14e364cb2a6a88ab83783d063",
            "b43d966235c24d528369292249cf2a5f",
            "cc2e72b0c83741ebb94830831be9eb8d",
            "7fc5661323a649f39049e9c37c93b3dc",
            "3b7090357986445ba1c752505eae6984",
            "9e3832000a4744ceb8d4c301bd672604",
            "34c93a87ac884d39945ea8990e9a96f9",
            "0cef774434234231b97a57d388ac0ab1",
            "e2f21720f8f5415e98b5e5643e374c08",
            "32e3f3ed923b416a83b3dc91f6ca8bb7",
            "62d50488bba743a3a7337e5ddd862480",
            "8c0bdee8c2244b6f81332f5728ce6f30"
          ]
        },
        "outputId": "cb8134b5-98a3-4a77-8092-4bd217a9da14"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Wikipedia mini dataset...\n",
            "\n",
            "Dataset size: (3200, 1)\n",
            "Null values: {'passage': 0}\n",
            "After cleanup: (3200, 1)\n",
            "\n",
            "Sample passage:\n",
            " Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the southeastern part of South America.  It is home to 3.3 million people, of which 1.7 million live in the capital Montevideo and its metropolitan area. ...\n",
            "Total Q&A pairs: 918\n",
            "Example Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
            "Example Answer: yes\n",
            "Total chunks created: 1289\n",
            "Example chunk:\n",
            " Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the southeastern part of South America.  It is home to 3.3 million people, of which 1.7 million live in the capital Montevideo and its metropolitan area. ...\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "438938a21b5943678d22b0cb26e041ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: (1289, 384)\n",
            "Setting up Milvus collection...\n",
            "Inserted entities: 2578\n",
            "Index ready.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FLAN-T5 ready âœ…\n",
            "ðŸ” Query: What are the three sections of a beetle?\n",
            "ID: 1281 | Score: 0.3709\n",
            "Text: s as generally assumed, which would necessitate splitting the traditional Pelecaniformes in three. ...\n",
            "\n",
            "ID: 1274 | Score: 0.3135\n",
            "Text: The Megadyptes - Eudyptes clade occurs at similar latitudes (though not as far north as the Galapagos Penguin), has its highest diversity in the New Z ...\n",
            "\n",
            "Q: Was Abraham Lincoln the sixteenth President of the United States?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Generated Answer ===\n",
            "Abraham Lincoln (February 12, 1809 Ã¢ April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassination.\n",
            "\n",
            "=== Retrieved Chunks ===\n",
            "[1] ID: 339 | Score: 0.7095\n",
            "Young Abraham Lincoln ...\n",
            "\n",
            "[2] ID: 320 | Score: 0.6434\n",
            "Abraham Lincoln (February 12, 1809 Ã¢Â€Â“ April 15, 1865) was the sixteenth President of the United States, serving from Ma ...\n",
            "\n",
            "[3] ID: 381 | Score: 0.5896\n",
            "On November 6, 1860, Lincoln was elected as the 16th President of the United States, beating Democrat Stephen A. Douglas ...\n",
            "\n",
            "QA evaluation size: 918\n",
            "Sample QA: {'question': 'Was Abraham Lincoln the sixteenth President of the United States?', 'answer': 'yes'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Using local Transformers: google/flan-t5-base\n",
            "instruction (20 samples): {'exact_match': 30.0, 'f1': 44.55011655011655}\n",
            "cot (20 samples): {'exact_match': 15.0, 'f1': 27.979597562930895}\n",
            "persona (20 samples): {'exact_match': 30.0, 'f1': 43.641025641025635}\n",
            "\n",
            "â†’ Evaluating instruction on 100 samples (GPU)\n",
            "\n",
            "â†’ Evaluating cot on 100 samples (GPU)\n",
            "\n",
            "â†’ Evaluating persona on 100 samples (GPU)\n",
            "\n",
            "=== Results ===\n",
            "instruction {'exact_match': 17.0, 'f1': 20.7685332211648}\n",
            "cot {'exact_match': 7.0, 'f1': 13.376291029729087}\n",
            "persona {'exact_match': 20.0, 'f1': 23.634334086965666}\n",
            "OK: datasets\n",
            "OK: sentence_transformers\n",
            "OK: transformers\n",
            "OK: faiss\n",
            "OK: numpy\n",
            "OK: pandas\n",
            "OK: evaluate\n",
            "Active configuration: {'encoder_model': 'sentence-transformers/all-MiniLM-L6-v2', 'chunk_size_chars': 600, 'embed_batch': 64, 'retrieval_candidates': 20, 'n_query_vectors': 3, 'rerank_top_k': 5, 'context_limit': 2000, 'use_openai_api': False}\n",
            "Corpus loaded: DatasetDict({\n",
            "    passages: Dataset({\n",
            "        features: ['passage', 'id'],\n",
            "        num_rows: 3200\n",
            "    })\n",
            "})\n",
            "QA loaded: DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['question', 'answer', 'id'],\n",
            "        num_rows: 918\n",
            "    })\n",
            "})\n",
            "Total text chunks: 1289\n",
            "Example: 0-0 Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43d966235c24d528369292249cf2a5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Index ready | dimension=384 | vectors=1289\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â“ Question: Was Abraham Lincoln the sixteenth President of the United States?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¬ Answer:\n",
            " Abraham Lincoln (February 12, 1809 Ã¢ April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassination.\n",
            "\n",
            "ðŸ“š Citations: [{'id': np.str_('278-0'), 'score': 10.308937072753906}, {'id': np.str_('319-0'), 'score': 8.12199592590332}, {'id': np.str_('198-0'), 'score': -0.4494704604148865}, {'id': np.str_('383-0'), 'score': -0.7347864508628845}, {'id': np.str_('281-0'), 'score': -0.9723390936851501}]\n",
            "[OK] chunks=1289 | embedder=SentenceTransformer | faiss_ntotal=1289\n",
            "Sample: {'id': '0', 'text': 'Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the southeastern part of South America.  It is home to 3.3 million people, of which 1.7 million live in the capital Montevideo and its metropolitan area.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Local generator (Flan-T5)\n",
        "_GEN_TOK = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "_GEN_MDL = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "GEN = pipeline(\"text2text-generation\", model=_GEN_MDL, tokenizer=_GEN_TOK, device_map=\"auto\")\n",
        "\n",
        "# Cross-encoder reranker\n",
        "from sentence_transformers import CrossEncoder\n",
        "_CE_ID  = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "_CE_TOK = AutoTokenizer.from_pretrained(_CE_ID)\n",
        "RERANKER = CrossEncoder(_CE_ID, max_length=512)\n",
        "\n",
        "def _trim_tokens(text: str, tokenizer, max_len: int) -> str:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) > max_len:\n",
        "        ids = ids[:max_len]\n",
        "    return tokenizer.decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnjcPgAKjwDv",
        "outputId": "f1a0cdc4-be3b-4d75-f0df-f3df0e848249"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def faiss_search_topk(query: str, k: int = 5) -> List[Tuple[int, str, float]]:\n",
        "    qv = embed_model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "    D, I = faiss_index.search(qv, k)\n",
        "    out = []\n",
        "    for i in range(k):\n",
        "        idx = int(I[0][i])\n",
        "        score = float(D[0][i])\n",
        "        out.append((idx, chunk_store[idx][\"text\"], score))\n",
        "    return out"
      ],
      "metadata": {
        "id": "nzrG_eWx12Ec"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_respond(question: str):\n",
        "    \"\"\"Top-1 retrieval + constrained generation (offline).\"\"\"\n",
        "    hits = faiss_search_topk(question, k=1)\n",
        "    contexts = [hits[0][1]] if hits else [\"\"]\n",
        "    prompt = (\n",
        "        \"Answer using ONLY the context. If insufficient, say 'I don't know.'\\n\\n\"\n",
        "        f\"Context:\\n{contexts[0]}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    )\n",
        "    out = GEN(prompt, max_new_tokens=128)[0][\"generated_text\"].strip()\n",
        "    return out, contexts\n",
        "\n",
        "def ranked_respond(question: str, k_final: int = 5, ctx_char_budget: int = 2000):\n",
        "    \"\"\"\n",
        "    Advanced: gather candidates (via your Step5 if present), rerank with CrossEncoder, generate with Flan-T5.\n",
        "    \"\"\"\n",
        "    # 1) Candidate gather: use your Step5.gather_candidates if available, else FAISS top-20\n",
        "    if hasattr(mod_step5, \"gather_candidates\"):\n",
        "        cands, _rewrites = mod_step5.gather_candidates(question)\n",
        "        # expected format: [(idx, text, score_like), ...]\n",
        "    else:\n",
        "        base = faiss_search_topk(question, k=20)\n",
        "        cands = [(i, t, s) for (i, t, s) in base]\n",
        "\n",
        "    if not cands:\n",
        "        return \"I don't know.\", [\"\"]\n",
        "\n",
        "    # 2) Rerank with cross-encoder\n",
        "    q_trim = _trim_tokens(question, _CE_TOK, max_len=32)\n",
        "    pairs, idxs = [], []\n",
        "    for idx, _t, _s in cands:\n",
        "        p_trim = _trim_tokens(chunk_store[idx][\"text\"], _CE_TOK, max_len=480)\n",
        "        pairs.append((q_trim, p_trim))\n",
        "        idxs.append(idx)\n",
        "    scores = RERANKER.predict(pairs, convert_to_numpy=True, show_progress_bar=False, batch_size=64)\n",
        "    order = np.argsort(-scores)[:k_final]\n",
        "    chosen = [int(idxs[i]) for i in order]\n",
        "\n",
        "    # 3) Context assembly (use Step5.build_context_with_citations if available)\n",
        "    if hasattr(mod_step5, \"build_context_with_citations\"):\n",
        "        ctx_text, _cites = mod_step5.build_context_with_citations([(i, float(scores[j])) for j,i in enumerate(chosen)], budget=ctx_char_budget)\n",
        "    else:\n",
        "        ctx_text = \"\"\n",
        "        for i in chosen:\n",
        "            frag = chunk_store[i][\"text\"]\n",
        "            if len(ctx_text) + len(frag) + 2 <= ctx_char_budget:\n",
        "                ctx_text += (\"\\n\\n\" + frag) if ctx_text else frag\n",
        "\n",
        "    # 4) Generate answer\n",
        "    def _persona(ctx: str, q: str) -> str:\n",
        "        q_t = _trim_tokens(q, _GEN_TOK, 48)\n",
        "        c_t = _trim_tokens(ctx, _GEN_TOK, 460)\n",
        "        return (\n",
        "            \"You are a concise encyclopedia editor. Use ONLY the context; \"\n",
        "            \"if insufficient, reply 'I don't know.'\\n\\n\"\n",
        "            f\"Context:\\n{c_t}\\n\\nQuestion: {q_t}\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "    ans = GEN(_persona(ctx_text, question), max_new_tokens=256)[0][\"generated_text\"].strip()\n",
        "    ctx_list = [chunk_store[i][\"text\"] for i in chosen]\n",
        "    return ans, ctx_list"
      ],
      "metadata": {
        "id": "0qsKoU9d2gMR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_respond(question: str):\n",
        "    \"\"\"Top-1 retrieval + constrained generation (offline).\"\"\"\n",
        "    hits = faiss_search_topk(question, k=1)\n",
        "    contexts = [hits[0][1]] if hits else [\"\"]\n",
        "    prompt = (\n",
        "        \"Answer using ONLY the context. If insufficient, say 'I don't know.'\\n\\n\"\n",
        "        f\"Context:\\n{contexts[0]}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    )\n",
        "    out = GEN(prompt, max_new_tokens=128)[0][\"generated_text\"].strip()\n",
        "    return out, contexts\n",
        "\n",
        "def ranked_respond(question: str, k_final: int = 5, ctx_char_budget: int = 2000):\n",
        "    \"\"\"\n",
        "    Advanced: gather candidates (via your Step5 if present), rerank with CrossEncoder, generate with Flan-T5.\n",
        "    \"\"\"\n",
        "    # 1) Candidate gather: use your Step5.gather_candidates if available, else FAISS top-20\n",
        "    if hasattr(mod_step5, \"gather_candidates\"):\n",
        "        cands, _rewrites = mod_step5.gather_candidates(question)\n",
        "        # expected format: [(idx, text, score_like), ...]\n",
        "    else:\n",
        "        base = faiss_search_topk(question, k=20)\n",
        "        cands = [(i, t, s) for (i, t, s) in base]\n",
        "\n",
        "    if not cands:\n",
        "        return \"I don't know.\", [\"\"]\n",
        "\n",
        "    # 2) Rerank with cross-encoder\n",
        "    q_trim = _trim_tokens(question, _CE_TOK, max_len=32)\n",
        "    pairs, idxs = [], []\n",
        "    for idx, _t, _s in cands:\n",
        "        p_trim = _trim_tokens(chunk_store[idx][\"text\"], _CE_TOK, max_len=480)\n",
        "        pairs.append((q_trim, p_trim))\n",
        "        idxs.append(idx)\n",
        "    scores = RERANKER.predict(pairs, convert_to_numpy=True, show_progress_bar=False, batch_size=64)\n",
        "    order = np.argsort(-scores)[:k_final]\n",
        "    chosen = [int(idxs[i]) for i in order]\n",
        "\n",
        "    # 3) Context assembly (use Step5.build_context_with_citations if available)\n",
        "    if hasattr(mod_step5, \"build_context_with_citations\"):\n",
        "        ctx_text, _cites = mod_step5.build_context_with_citations([(i, float(scores[j])) for j,i in enumerate(chosen)], budget=ctx_char_budget)\n",
        "    else:\n",
        "        ctx_text = \"\"\n",
        "        for i in chosen:\n",
        "            frag = chunk_store[i][\"text\"]\n",
        "            if len(ctx_text) + len(frag) + 2 <= ctx_char_budget:\n",
        "                ctx_text += (\"\\n\\n\" + frag) if ctx_text else frag\n",
        "\n",
        "    # 4) Generate answer\n",
        "    def _persona(ctx: str, q: str) -> str:\n",
        "        q_t = _trim_tokens(q, _GEN_TOK, 48)\n",
        "        c_t = _trim_tokens(ctx, _GEN_TOK, 460)\n",
        "        return (\n",
        "            \"You are a concise encyclopedia editor. Use ONLY the context; \"\n",
        "            \"if insufficient, reply 'I don't know.'\\n\\n\"\n",
        "            f\"Context:\\n{c_t}\\n\\nQuestion: {q_t}\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "    ans = GEN(_persona(ctx_text, question), max_new_tokens=256)[0][\"generated_text\"].strip()\n",
        "    ctx_list = [chunk_store[i][\"text\"] for i in chosen]\n",
        "    return ans, ctx_list"
      ],
      "metadata": {
        "id": "HwOEaOjB2iuc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_ds = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")[\"test\"]\n",
        "M = min(SAMPLE_COUNT, len(qa_ds))\n",
        "print(\"Scoring with samples:\", M)\n",
        "\n",
        "def _best_reference_snippet(question: str, gold: str) -> str:\n",
        "    # Try exact match; else nearest via FAISS\n",
        "    g = (gold or \"\").strip()\n",
        "    if g:\n",
        "        pat = re.escape(g)\n",
        "        for d in chunk_store:\n",
        "            if re.search(pat, d[\"text\"], flags=re.IGNORECASE):\n",
        "                return d[\"text\"]\n",
        "    qv = embed_model.encode([question], normalize_embeddings=True).astype(\"float32\")\n",
        "    D, I = faiss_index.search(qv, 1)\n",
        "    return chunk_store[int(I[0][0])][\"text\"]\n",
        "\n",
        "def make_eval_dataset(which: str = \"basic\") -> Dataset:\n",
        "    rows, skipped = [], 0\n",
        "    for i in range(M):\n",
        "        q = qa_ds[i][\"question\"]\n",
        "        gt = qa_ds[i][\"answer\"] if \"answer\" in qa_ds[i] else qa_ds[i][\"answers\"]\n",
        "        gt_text = gt if isinstance(gt, str) else gt[0]\n",
        "\n",
        "        try:\n",
        "            if which == \"basic\":\n",
        "                a, ctxs = basic_respond(q)\n",
        "            else:\n",
        "                a, ctxs = ranked_respond(q, k_final=5, ctx_char_budget=2000)\n",
        "\n",
        "            ctxs = ctxs if isinstance(ctxs, list) else [str(ctxs)]\n",
        "            ref = _best_reference_snippet(q, gt_text)\n",
        "            row = {\n",
        "                \"question\": q,\n",
        "                \"answer\": str(a),\n",
        "                \"contexts\": [str(c) for c in ctxs] if ctxs else [\"\"],\n",
        "                \"ground_truths\": [gt_text],\n",
        "                \"reference\": str(ref),\n",
        "            }\n",
        "            rows.append(row)\n",
        "        except Exception:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "    if not rows:\n",
        "        raise RuntimeError(\"No samples builtâ€”check upstream notebooks.\")\n",
        "    print(f\"{which}: built {len(rows)} rows (skipped {skipped})\")\n",
        "    return Dataset.from_list(rows)\n",
        "\n",
        "ds_basic  = make_eval_dataset(\"basic\")\n",
        "ds_ranked = make_eval_dataset(\"ranked\")\n",
        "print(\"Sample row:\", ds_basic[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KXODOU62oCL",
        "outputId": "0aff2b5e-a9cc-42d6-e38d-17ca31d62728"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring with samples: 50\n",
            "basic: built 50 rows (skipped 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ranked: built 50 rows (skipped 0)\n",
            "Sample row: {'question': 'Was Abraham Lincoln the sixteenth President of the United States?', 'answer': \"I don't know\", 'contexts': ['Young Abraham Lincoln'], 'ground_truths': ['yes'], 'reference': \"At the age of twenty, in 1812, at the end of his apprenticeship, Faraday attended lectures by the eminent English chemist and physicist Humphry Davy of the Royal Institution and Royal Society, and John Tatum, founder of the City Philosophical Society. Many tickets for these lectures were given to Faraday by William Dance (one of the founders of the Royal Philharmonic Society). Afterwards, Faraday sent Davy a three hundred page book based on notes taken during the lectures. Davy's reply was immediate, kind, and favorable. When Davy damaged his eyesight in an accident with nitrogen trichloride, \"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map chunk text â†’ synthetic id for overlap evaluation\n",
        "_text_to_id = {d[\"text\"]: str(d.get(\"id\", i)) for i, d in enumerate(chunk_store)}\n",
        "\n",
        "def _text_to_nearest_id(txt: str) -> str:\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    if txt in _text_to_id:\n",
        "        return _text_to_id[txt]\n",
        "    snippet = txt[:1200]\n",
        "    vec = embed_model.encode([snippet], normalize_embeddings=True).astype(\"float32\")\n",
        "    D, I = faiss_index.search(vec, 1)\n",
        "    return str(int(I[0][0]))\n",
        "\n",
        "def _rows_to_pairs(ds: Dataset):\n",
        "    rows = []\n",
        "    for r in ds:\n",
        "        ctx_ids = [_text_to_nearest_id(t) for t in (r.get(\"contexts\") or []) if t]\n",
        "        ref_id  = _text_to_nearest_id(r.get(\"reference\",\"\"))\n",
        "        if ctx_ids and ref_id:\n",
        "            rows.append({\"retrieved\": ctx_ids, \"reference\": [ref_id]})\n",
        "    if not rows:\n",
        "        raise RuntimeError(\"No valid rows for scoring.\")\n",
        "    return rows\n",
        "\n",
        "pairs_basic  = _rows_to_pairs(ds_basic)\n",
        "pairs_ranked = _rows_to_pairs(ds_ranked)\n",
        "\n",
        "def score_overlap(pairs):\n",
        "    precisions, recalls = [], []\n",
        "    inter_total = ret_total = ref_total = 0\n",
        "    for p in pairs:\n",
        "        R = set(p[\"retrieved\"])\n",
        "        G = set(p[\"reference\"])\n",
        "        inter = len(R & G)\n",
        "        precisions.append(inter / len(R) if R else 0.0)\n",
        "        recalls.append(inter / len(G) if G else 0.0)\n",
        "        inter_total += inter\n",
        "        ret_total   += len(R)\n",
        "        ref_total   += len(G)\n",
        "    return {\n",
        "        \"context_precision_macro\": float(np.mean(precisions)) if precisions else 0.0,\n",
        "        \"context_recall_macro\": float(np.mean(recalls)) if recalls else 0.0,\n",
        "        \"context_precision_micro\": float(inter_total / ret_total) if ret_total else 0.0,\n",
        "        \"context_recall_micro\": float(inter_total / ref_total) if ref_total else 0.0,\n",
        "        \"n_samples_scored\": len(pairs),\n",
        "    }\n",
        "\n",
        "scores_basic  = score_overlap(pairs_basic)\n",
        "scores_ranked = score_overlap(pairs_ranked)\n",
        "\n",
        "df_out = pd.DataFrame(\n",
        "    [{\"system\":\"basic\", **scores_basic}, {\"system\":\"ranked\", **scores_ranked}]\n",
        ").set_index(\"system\")\n",
        "display(df_out)\n",
        "\n",
        "df_out.reset_index().to_csv(CSV_OUT, index=False)\n",
        "print(\"Saved metrics to:\", CSV_OUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "nJHfnBMP2xo3",
        "outputId": "43c6e0c4-4512-4304-c462-93ce49ee3a6a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        context_precision_macro  context_recall_macro  \\\n",
              "system                                                  \n",
              "basic                     0.160                  0.16   \n",
              "ranked                    0.048                  0.24   \n",
              "\n",
              "        context_precision_micro  context_recall_micro  n_samples_scored  \n",
              "system                                                                   \n",
              "basic                     0.160                  0.16                50  \n",
              "ranked                    0.048                  0.24                50  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d00c4565-c0d9-4a86-9d01-55bff1099818\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_precision_macro</th>\n",
              "      <th>context_recall_macro</th>\n",
              "      <th>context_precision_micro</th>\n",
              "      <th>context_recall_micro</th>\n",
              "      <th>n_samples_scored</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>system</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>basic</th>\n",
              "      <td>0.160</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.16</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ranked</th>\n",
              "      <td>0.048</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.24</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d00c4565-c0d9-4a86-9d01-55bff1099818')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d00c4565-c0d9-4a86-9d01-55bff1099818 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d00c4565-c0d9-4a86-9d01-55bff1099818');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-32b6d7b2-080c-4bb7-9dd0-0758e4e6e2c0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-32b6d7b2-080c-4bb7-9dd0-0758e4e6e2c0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-32b6d7b2-080c-4bb7-9dd0-0758e4e6e2c0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_72e9effa-0807-4dae-b951-a06c49591bd4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_out')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_72e9effa-0807-4dae-b951-a06c49591bd4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_out');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_out",
              "summary": "{\n  \"name\": \"df_out\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"system\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"ranked\",\n          \"basic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07919595949289332,\n        \"min\": 0.04800000000000001,\n        \"max\": 0.16,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.04800000000000001,\n          0.16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05656854249492379,\n        \"min\": 0.16,\n        \"max\": 0.24,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.24,\n          0.16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision_micro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07919595949289332,\n        \"min\": 0.048,\n        \"max\": 0.16,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.048,\n          0.16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall_micro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05656854249492379,\n        \"min\": 0.16,\n        \"max\": 0.24,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.24,\n          0.16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_samples_scored\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 50,\n        \"max\": 50,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          50\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics to: /content/drive/MyDrive/NLX_LLM_Project_2/artifacts/step6_offline_overlap_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL â€” run only if you want ragas metrics\n",
        "from ragas import evaluate as ragas_evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
        "\n",
        "# Minimal HF adapter (ragas may call an LLM; we point it to Flan-T5)\n",
        "class SimpleHFLLM:\n",
        "    name = \"local-hf-flan-t5\"\n",
        "    def __init__(self, gen_pipe, default_max_new_tokens: int = 64):\n",
        "        self.pipe = gen_pipe\n",
        "        self._max = default_max_new_tokens\n",
        "    def set_run_config(self, run_config):  # ragas compatibility\n",
        "        pass\n",
        "    def _to_text(self, p):\n",
        "        try:\n",
        "            if hasattr(p, \"to_string\"): return p.to_string()\n",
        "            if hasattr(p, \"text\"):      return str(p.text)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return str(p)\n",
        "    def generate(self, prompts, **kwargs):\n",
        "        mx = int(kwargs.get(\"max_new_tokens\", self._max))\n",
        "        outs = []\n",
        "        for p in prompts:\n",
        "            txt = self._to_text(p)\n",
        "            outs.append(self.pipe(txt, max_new_tokens=mx)[0][\"generated_text\"])\n",
        "        return outs\n",
        "    async def agenerate(self, prompts, **kwargs):\n",
        "        return self.generate(prompts, **kwargs)\n",
        "    def generate_prompt(self, prompts, **kwargs):\n",
        "        return self.generate(prompts, **kwargs)\n",
        "    async def agenerate_prompt(self, prompts, **kwargs):\n",
        "        return self.agenerate(prompts, **kwargs)\n",
        "\n",
        "local_llm = SimpleHFLLM(GEN)\n",
        "\n",
        "# Choose which dataset to evaluate: ds_basic or ds_ranked\n",
        "ragas_result = ragas_evaluate(\n",
        "    ds_ranked,  # or ds_basic\n",
        "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
        "    llm=local_llm\n",
        ")\n",
        "print(ragas_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "KFhKYdLM5_bc",
        "outputId": "d4940bd1-a858-4e4f-ebf0-df6f9da3828f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2098119720.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Choose which dataset to evaluate: ds_basic or ds_ranked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m ragas_result = ragas_evaluate(\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mds_ranked\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# or ds_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfaithfulness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_relevancy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_recall\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/_analytics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor, allow_nest_asyncio)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_async_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/async_utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(async_func, allow_nest_asyncio)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# Create the coroutine if it's a callable, otherwise use directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mcoro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masync_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0masync_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36m_async_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# Create async wrapper for aevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_async_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         return await aevaluate(\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36maevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetricWithEmbeddings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0membeddings_changed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/embeddings/base.py\u001b[0m in \u001b[0;36membedding_factory\u001b[0;34m(provider, model, run_config, client, interface, base_url, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"text-embedding-ada-002\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         )\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mopenai_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mopenai_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/embeddings/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_proxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_proxy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_async_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k2xNBS8j6DCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}